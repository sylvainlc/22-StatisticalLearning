\documentclass[a4paper,10pt,fleqn]{article}

\usepackage{a4wide,amsmath,amsthm,amssymb,bbm,fancyhdr}
\usepackage{ifthen,color,enumerate,comment,dsfont,pdfsync,framed,todonotes,enumitem}

\newcommand{\titre}[1]{\textbf{\textsc{#1}}}

\RequirePackage[T1]{fontenc}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{enumitem,url,hyperref}
\newcommand{\eqsp}{\,}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\rme}{\ensuremath{\mathrm{e}}}
\newcommand{\calH}{\ensuremath{\mathcal{H}}}
\newcommand{\xset}{\ensuremath{\mathsf{X}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\Sb}{\ensuremath{\mathbb{S}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\HH}{\ensuremath{\mathcal{H}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\W}{\ensuremath{\mathcal{W}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bE}{\mathbb{E}}

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\hatk}{\widehat K}
\newcommand{\f}{\varphi}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfs}{\mathbf{\Sigma}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\crit}{\mathop{\textrm{crit}}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\pc}{\pi_{\mathcal{C}}}
\newcommand{\param}{\theta}

% Style


\begin{document}

%\noindent EMINES \hfill \\%ISUP - Sorbonne Universit\'e \\
 %2022-2023

\noindent\hrulefill

\begin{center}
\textsc{Ridge regression}
\end{center}
\hrulefill

\medskip


\section*{Generalized Ridge regression}
Consider the regression model
$$
Y = X\beta_* + \varepsilon\,,
$$
where $X\in\mathbb{R}^{n\times d}$, $\beta_*$ is an unknown vector in $\mathbb{R}^{d}$ and $\varepsilon\sim \mathcal{N}(0,\sigma^2 I_n)$.
Define the generalized Ridge estimator by:
$$
\widehat{\beta} \in \mathrm{Argmin}_{\beta\in \mathbb{R}^{d}} \left\{(Y -X\beta)^\top W (Y-X\beta) + (\beta-\beta_0)^\top \Delta (\beta-\beta_0)\right\}\,,
$$
where $\beta_0 \in \mathbb{R}^{d}$, $W\in\rset^{n\times n}$ is a diagonal matrix with elements in $[0,1]$, $\Delta\in\rset^{d\times d}$ is a symmetric definite-positive matrix.
\begin{enumerate}
\item Provide the expression of $\widehat \beta$ when $\beta_0 = 0$, $W = I_n$ and $\Delta = \lambda I_d$ where $\lambda>0$.

\vspace{.2cm}

{\em
Proof in lecture notes.
} 
\item Solve the optimization problem in the general case.
\vspace{.2cm}

{\em
For all $\beta\in\rset^d$, write
$$
\mathcal{L}(\beta) = (Y -X\beta)^\top W (Y-X\beta) + (\beta-\beta_0)^\top \Delta (\beta-\beta_0)\eqsp.
$$
Therefore, for all $\beta\in\rset^d$,
$$
\nabla \mathcal{L}(\beta)  = 2\left( \left(X^\top W X + \Delta\right)\beta - \Delta\beta_0 - X^\top W Y\right)\eqsp.
$$
Note that $X^\top W X + \Delta$ is definite-positive so that $\nabla \mathcal{L}(\beta)=0$ has a unique solution given by
$$
\widehat{\beta} = \left(X^\top W X + \Delta\right)^{-1}(\Delta\beta_0 + X^\top W Y)\eqsp.
$$
}
\item Compute $\mathbb{E}[\widehat \beta]$ and show that the estimator is unbiased when $\beta_0 = \beta_*$.
\vspace{.2cm}

{\em
Assuming that the design is not random, 
$$
\mathbb{E}[\widehat \beta] =  \left(X^\top W X + \Delta\right)^{-1}(\Delta\beta_0 + X^\top W \mathbb{E}[Y])\eqsp.
$$
This yields
$$
\mathbb{E}[\widehat \beta] =  \left(X^\top W X + \Delta\right)^{-1}(\Delta\beta_0 + X^\top W X\beta_*)\eqsp.
$$
In the case where $\beta_0 = \beta_*$,
$$
\mathbb{E}[\widehat \beta] =  \left(X^\top W X + \Delta\right)^{-1}\left(X^\top W X + \Delta\right)\beta_* = \beta_*
$$
and the estimator is unbiased.
}
\item Compute $\mathbb{V}[\widehat \beta]$ and the mean squared error $\mathbb{E}[\|\widehat \beta-\beta_*\|_2^2]$ when $\beta_0 = \beta_*$.
\vspace{.2cm}

{\em
By definition of $\widehat \beta$,
\begin{align*}
\mathbb{V}[\widehat \beta] &= \left(X^\top W X + \Delta\right)^{-1}X^\top W \mathbb{V}[Y] W^\top X\left(X^\top W X + \Delta\right)^{-1}\\
&= \sigma^2\left(X^\top W X + \Delta\right)^{-1}X^\top W W^\top X\left(X^\top W X + \Delta\right)^{-1}\\
&= \sigma^2\left(X^\top W X + \Delta\right)^{-1}X^\top W^2 X\left(X^\top W X + \Delta\right)^{-1}\eqsp.
\end{align*}
If  $\beta_0 = \beta_*$, as the estimator is unbiased,
\begin{align*}
\mathbb{E}[\|\widehat \beta-\beta_*\|_2^2] &= \mathrm{Trace}\left(\mathbb{V}[\widehat \beta] \right)\\
&=\sigma^2\mathrm{Trace}\left(\left(X^\top W X + \Delta\right)^{-1}X^\top W^2 X\left(X^\top W X + \Delta\right)^{-1}\right)\\
&=\sigma^2\mathrm{Trace}\left(X^\top W^2 X\left(X^\top W X + \Delta\right)^{-2}\right)\eqsp.
\end{align*}
}
\item Assume that $W=I_n$, $\beta_0=0$ and $\Delta = V\Lambda 
 V^\top$ where $X = U D V^\top$ is a singular value decomposition of $X$ and $\Lambda$ is a diagonal matrix with positive diagonal components. Provide an expression of $\widehat \beta$ as a function of $U$, $D$, $V$, $\Lambda$ and $Y$.
\vspace{.2cm}

{\em
In the proposed setting,
$$
\widehat{\beta} = \left(X^\top X + \Delta\right)^{-1}X^\top Y\eqsp.
$$
Let $X = U D V^\top$ be a singular value decomposition of $X$ and choose $\Delta = V\Lambda 
 V^\top$. Then,
\begin{align*}
\widehat{\beta} &= \left((U D V^\top)^\top U D V^\top + V\Lambda V^\top\right)^{-1}(U D V^\top)^\top Y\\
&=  \left(V D^\top U^\top U D V^\top + V\Lambda V^\top\right)^{-1}V D^\top U^\top Y\\
&=  V\left(D^\top D + \Lambda \right)^{-1}D^\top U^\top Y\eqsp.
\end{align*}
Contrary to the classical Ridge estimator, this estimator shrinks values of $\beta$ with a different penalty for each component thanks to the matrix $\Lambda$.
}
\end{enumerate}



\end{document}





	
