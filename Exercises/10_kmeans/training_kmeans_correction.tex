\documentclass[a4paper,10pt,fleqn]{article}

\usepackage{a4wide,amsmath,amsthm,amssymb,bbm,fancyhdr}
\usepackage{ifthen,color,enumerate,comment,dsfont,pdfsync,framed,todonotes,enumitem}
\newboolean{corrige}
\setboolean{corrige}{true}

\newcommand{\titre}[1]{\textbf{\textsc{#1}}}

\RequirePackage[T1]{fontenc}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{dsfont}
\newcommand{\thisyear}{}
\usepackage{enumitem}
\newcommand{\eqsp}{\,}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\rme}{\ensuremath{\mathrm{e}}}
\newcommand{\calH}{\ensuremath{\mathcal{H}}}
\newcommand{\xset}{\ensuremath{\mathsf{X}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\Sb}{\ensuremath{\mathbb{S}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\HH}{\ensuremath{\mathcal{H}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\W}{\ensuremath{\mathcal{W}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\hatk}{\widehat K}
\newcommand{\f}{\varphi}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfs}{\mathbf{\Sigma}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\crit}{\mathcal{L}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\pc}{\pi_{\mathcal{C}}}


% Style
%\pagestyle{fancyplain}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
%\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\footnotesize {\sf
%MAT4506 Introduction to Machine Learning  %/ \rightmark
%}}}
%\rhead[\fancyplain{}{\footnotesize {\sf MAT4506 Introduction to machine learning, \thisyear %/ \rightmark
%}}]{\fancyplain{}{\thepage}}


\newtheorem{theorem}{Theorem}


\begin{document}

\noindent\hrulefill

\begin{center}
\textsc{Unsupervised learning}
\end{center}
\hrulefill

\medskip


\section{K-means algorithm}
Let $n\geqslant 1$ and  $X_{1},\ldots,X_{n}$ in $\R^d$.
The $K$-means algorithm aims at minimizing over all partitions $G=(G_{1},\ldots,G_{K})$ of $\{1,\ldots,p\}$ the criterion
$$
\crit(G)=\sum_{k=1}^K\sum_{i\in G_{k}}\|X_{i}-\bar{X}_{G_{i}}\|^2\quad \textrm{with}\quad \bar{X}_{G_{k}}={1\over |G_{k}|}\sum_{a\in G_{k}} X_{a}\;.
$$
\begin{enumerate}
\item Prove that
$$
\crit(G)=\sum_{k=1}^K{1\over |G_{k}|}\sum_{a,b\in G_{k}} \langle X_{a},X_{a}-X_{b}\rangle ={1\over 2}\sum_{k=1}^K{1\over |G_{k}|}\sum_{a,b\in G_{k}} \|X_{a}-X_{b}\|^2\;.
$$

\vspace{.2cm}

{\em
By definition,
\begin{align*}
\crit(G) & = \sum_{k=1}^K\sum_{a\in G_{k}}\|X_{a}-\bar{X}_{G_{k}}\|^2 \\
& = \sum_{k=1}^K \sum_{a\in G_{k}} \langle X_a - \frac{1}{|G_k|} \sum_{b \in G_k} X_b,X_a - \frac{1}{|G_k|} \sum_{c \in G_k} X_c \rangle \\
& = \sum_{k=1}^K \frac{1}{|G_k|^2}\sum_{a, b, c\in G_{k}} \langle X_a - X_b,X_a - X_c \rangle \\
& = \sum_{k=1}^K \frac{1}{|G_k|^2}\sum_{a, b, c\in G_{k}} \langle X_a - X_b,X_a \rangle 
- \sum_{k=1}^K \frac{1}{|G_k|^2}\sum_{a, b, c\in G_{k}} \langle X_a - X_b,X_c \rangle,
\end{align*}
where
$$
\sum_{a, b, c\in G_{k}} \langle X_a - X_b,X_c \rangle  = |G_k| \sum_{a, c\in G_{k}} \langle X_a ,X_c \rangle - |G_k| \sum_{b, c\in G_{k}} \langle X_b ,X_c \rangle = 0\eqsp.
$$
Thus, 
\begin{align*}
\crit(G) & = \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \langle X_a,X_a -X_b \rangle.
\end{align*}
For the second equality, note that 
\begin{align*}
\crit(G) & = \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \langle X_a-X_b,X_a -X_b \rangle + \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \langle X_b,X_a -X_b \rangle\\
& = \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \| X_a-X_b \|^2 - \crit(G),
\end{align*}
which concludes the proof. 
}
\item Assume now that the observations are independent. Write $\E[X_a] = \mu_{a}\in\R^d$ so that $X_{a}=\mu_{a}+\eps_{a}$ with $\eps_{1},\ldots,\eps_{n}$ centered and independent. Define $v_{a}=\textrm{trace}(\V[X_{a}])$. Prove that
$$
\E[\crit(G)]={1\over 2}\sum_{k=1}^K{1\over |G_{k}|}\sum_{a,b\in G_{k}} \left(\|\mu_{a}-\mu_{b}\|^2+v_{a}+v_{b}\right){\bf 1}_{a\neq b}\;.
$$
What is the value of $\E[\crit(G)]$ when all the within-group variables have the same mean?

\vspace{.2cm}

{\em

The expectation of $\crit(G)$ is given by
\begin{align*}
\E \left[ \crit(G) \right] & = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \E \left[ \| X_a-X_b \|^2 \right].
\end{align*}
Let $a, b \in G_k, a \neq b$,
\begin{align*}
\E \left[ \| X_a-X_b \|^2 \right] & = \E \left[ \| \mu_a - \mu_b + \varepsilon_a - \varepsilon_b \|^2\right]\\
& = \E \left[ \| \mu_a - \mu_b\|^2 \right]   + \E \left[ \| \varepsilon_a - \varepsilon_b \|^2 \right] 
+ 2  \E \left[  \langle \mu_a - \mu_b, \varepsilon_a - \varepsilon_b \rangle \right]\\
& = \| \mu_a - \mu_b\|^2 + \E \left[ \| \varepsilon_a \|^2 \right] + \E \left[ \| \varepsilon_b \|^2 \right] + 2 \E \left[ \langle \varepsilon_a, \varepsilon_b \rangle \right],
\end{align*}
since $\varepsilon_a$ and $\varepsilon_b$ are independent and centred. Finally, since for all $a \in G_k, \E \left[ \| \varepsilon_a \|^2 \right] = v_a$, 
\begin{align*}
\E \left[ \crit(G) \right] & = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( \| \mu_a - \mu_b\|^2 + v_a + v_b \right) \mathds{1}_{a \neq b}.
\end{align*}
If all the within-group variables have the same mean, for all $k$, there exists $\mu_k$ such that, for all $a \in G_k$, $\mu_a = \mu_k$. Therefore, 
\begin{align*}
\E \left[ \crit(G) \right] & = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( v_a + v_b \right) \mathds{1}_{a \neq b}\\
& = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( v_a + v_b \right) \mathds{1}_{a \neq b},
\end{align*}
where 
\begin{align*}
\frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( v_a + v_b \right) \mathds{1}_{a \neq b} &= \frac{1}{|G_k|} \left( \sum_{a, b\in G_{k}} \left( v_a + v_b \right)  - \sum_{a, b\in G_{k}} \left( v_a + v_b \right) \mathds{1}_{a = b} \right) \\
& = \frac{1}{|G_k|} \left( 2 |G_k| \sum_{a \in G_{k}} v_a - 2 \sum_{a \in G_{k}} v_a \right)\\
& = \frac{2 (|G_k|-1)}{|G_k|} \sum_{a \in G_{k}} v_a.
\end{align*}
Consequently, if, for all $a \in G_k$, $\mu_a = \mu_k$, we have
$$
\E \left[ \crit(G) \right]  =\sum_{k=1}^K \frac{|G_k|-1}{|G_k|} \sum_{a \in G_{k}} v_a.
$$
}
\item We assume now that there exists a partition $G^*=(G^*_{1},\ldots,G^*_{K})$ such that there exist $m_{1},\ldots,m_{K}\in\R^d$ and $\gamma_{1},\ldots,\gamma_{K}>0$ satisfying $\mu_{a}=m_{k}$ and $v_{a}=\gamma_{k}$ for all $a\in G^*_{k}$ and $k=1,\ldots,K$. Compute $\E[\crit(G^*)]$.

\vspace{.2cm}

{\em
By definition of  $G^*$,  
	\begin{align*}
	\E \left[ \crit(G^*) \right] & =\sum_{k=1}^K \frac{|G^*_k|-1}{|G^*_k|} \sum_{a \in G^*_{k}} v_a\\
	& = \sum_{k=1}^K \frac{|G^*_k|-1}{|G^*_k|} |G^*_k| \gamma_k\\
	& = \sum_{k=1}^K (|G^*_k|-1) \gamma_k.
	\end{align*}
}
\item In the special case where there exists $\gamma > 0$ such that $v_{i}=\gamma$ for all $i \in \{1,\ldots,n\}$, which partition $G=(G_{1},\ldots,G_{K})$ minimizes $\E[\crit(G)]$?

\vspace{.2cm}

{\em
Assume that $v_a = \gamma$ for all $a$. Then, for any partition $G$, 
	\begin{align*}
	\E \left[ \crit(G) \right] & = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( \| \mu_a - \mu_b\|^2 \right)
	+ \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( v_a + v_b \right) \mathds{1}_{a \neq b}\\
	& = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( \| \mu_a - \mu_b\|^2 \right) + \sum_{k=1}^K \frac{|G_k|-1}{|G_k|} \sum_{a \in G_k} \gamma\\
	& = \frac{1}{2}  \sum_{k=1}^K \frac{1}{|G_k|}\sum_{a, b\in G_{k}} \left( \| \mu_a - \mu_b\|^2 \right) + \gamma(n-K) \\
	& \geqslant \gamma(n-K).
	\end{align*}
	In particular, for $G^*$ we have $\E \left[ \crit(G^*) \right] = \gamma(n-K)$.
	Therefore, the minimum of $\E \left[ \crit(G) \right]$ is reached at $G = G^*$.
	To prove that this minimum is unique when all $\mu_k$ are different, choose $G$ such that $\E \left[ \crit(G) \right] = \E \left[ \crit(G^*) \right]$.
	Then, for all $k$, and for all $a,b \in G_k$,  $\mu_a = \mu_b$ which implies that $G = G^*$.
}
\end{enumerate}
	
\section{EM algorithm (bonus)}
In the case where we are interested in estimating unknown parameters $\theta\in\mathbb{R}^m$ characterizing a model with missing data, the Expectation Maximization (EM) algorithm (Dempster et al. 1977) can be used when the joint distribution of the missing data $X$ and the observed data $Y$ is explicit. For all $\theta\in\mathbb{R}^m$, let $p_{\theta}$ be the probability density function of $(X,Y)$ when the model is parameterized by $\theta$ with respect to a given reference measure $\mu$. The EM algorithm aims at computing iteratively an approximation of the maximum likelihood estimator which maximizes the observed data loglikelihood:
$$
\ell(\theta;Y) = \log p_{\theta}(Y) =\log \int f_{\theta}(x,Y)\mu(\mathrm{d}x)\eqsp.
$$
As this quantity cannot be computed explicitly in general cases, the EM algorithm finds the maximum likelihood estimator by iteratively maximizing the expected complete data loglikelihood.
%Denote by 
%$$
%\ell(X,Y;\theta) = \log f(X,Y;\theta)
%$$
%the complete data loglikelihood  
Start with an inital value $\theta^{(0)}$ and let $\theta^{(t)}$ be the estimate at the $t$-th iteration for $t\geqslant 0$, then the next iteration of EM is decomposed into two steps.
\begin{enumerate}
\item {\bf E step}. Compute the expectation of the complete data loglikelihood, with respect to the conditional distribution of the missing data given the observed data parameterized by $\theta^{(t)}$:
$$
Q(\theta,\theta^{(t)}) =\mathbb{E}_{\theta^{(t)}}\left[\log p_{\theta}(X,Y)|Y \right]\eqsp.%=\int \ell(x,Y;\theta)f(x|Y;\theta^{(t)}) \mathrm{d}x \eqsp.
$$
\item {\bf M step}. Determine $\theta^{(t+1)}$ by maximizing the function Q:
$$
\theta^{(t+1)}\in \mbox{argmax}_\theta Q(\theta,\theta^{(t)})\eqsp.
$$
\end{enumerate}
\begin{enumerate}
\item Prove the following crucial property motivates the EM algorithm.  For all $\theta,\theta^{(t)}$,
$$
\ell(Y;\theta) - \ell(Y;\theta^{(t)}) \geqslant Q(\theta,\theta^{(t)})-Q(\theta^{(t)},\theta^{(t)})\eqsp.
$$


\vspace{.2cm}

{\em
This may be proved by noting that
$$
\ell(Y;\theta) = \log \left(\frac{p_{\theta}(X,Y)}{p_{\theta}(X|Y)}\right)\eqsp.
$$
Considering the conditional expectation of both terms given $Y$ when the parameter value is $\theta^{(t)}$ yields
$$
\ell(Y;\theta) = Q(\theta,\theta^{(t)}) - \mathbb{E}_{\theta^{(t)}}[\log p_{\theta}(X|Y)|Y]\eqsp.
$$
Then,
$$
\ell(Y;\theta) - \ell(Y;\theta^{(t)}) = Q(\theta,\theta^{(t)})-Q(\theta^{(t)},\theta^{(t)}) + H(\theta,\theta^{(t)}) - H(\theta^{(t)},\theta^{(t)})\eqsp,
$$
where
$$
H(\theta,\theta^{(t)}) = - \mathbb{E}_{\theta^{(t)}}[\log p_{\theta}(X|Y)|Y]\eqsp.
$$
The proof is completed by noting that
$$
H(\theta,\theta^{(t)}) - H(\theta^{(t)},\theta^{(t)})\geqslant 0\eqsp,
$$
as this difference if a Kullback-Leibler divergence. 
}
%Therefore, any value $\theta$ ,which improves $Q(\theta,\theta^{(t)})$ beyond reference value $Q(\theta^{(t)},\theta^{(t)})$, won't decrease the observed-data likelihood. Based on this inequality, the EM algorithm produces iteratively a sequence of parameter estimates $\left(\theta^{(t)}\right)_{t\ge 0}$. 
\end{enumerate}
In the following, $X = (X_1,\ldots,X_n)$ and $Y = (Y_1,\ldots,Y_n)$ where $\{(X_i,Y_i)\}_{1\leqslant i\leqslant n}$  are i.i.d. in $\{-1,1\} \times \rset^d$. For $k\in\{-1,1\}$, write $\pi_k = \bP(X_1 = k)$. Assume that, conditionally on the event $\{X_1 = k\}$, $Y_1$ has a Gaussian distribution with mean $\mu_k \in\rset^d$ and covariance matrix $\Sigma\in \rset^{d\times d}$. In this case, the parameter $\theta=(\pi_1, \mu_1,\mu_{-1}, \Sigma)$ belongs to the set $\Theta= [0,1] \times \rset^d \times \rset^d \times \rset^{d \times d}$.
\begin{enumerate}
\item Write the complete data loglikelihood.

\vspace{.2cm}

{\em
The complete data loglikelihood  is given by
\begin{align*}
\log p_{\theta}\left(X,Y\right) &= - \frac{nd}{2} \log(2\pi)+\sum_{i=1}^n\sum_{k\in\{-1,1\}}\1_{X_i=k}\left(\log \pi_{k} -\frac{\log \det \Sigma}{2} - \frac{1}{2}\left(Y_i - \mu_{k}\right)^\top\Sigma^{-1}\left(Y_i - \mu_{k}\right)\right) \eqsp,\\
&= - \frac{nd}{2} \log(2\pi)-\frac{n}2 \log\det\Sigma + \left(\sum_{i=1}^n\1_{X_i=1}\right)\log \pi_1 + \left(\sum_{i=1}^n\1_{X_i=-1}\right)\log (1-\pi_{1})\\
& \hspace{1cm}-  \frac{1}{2}\sum_{i=1}^n\1_{X_i=1}\left(Y_i - \mu_{1}\right)^\top\Sigma^{-1}\left(Y_i - \mu_{1}\right) -  \frac{1}{2}\sum_{i=1}^n\1_{X_i=-1}\left(Y_i - \mu_{-1}\right)^\top\Sigma^{-1}\left(Y_i - \mu_{-1}\right)\eqsp.
\end{align*}
}
\item Let $\theta^{(t)}$ be the current parameter estimate. Compute $\theta\mapsto Q(\theta,\theta^{(t)})$.

\vspace{.2cm}

{\em
Write $\omega_t^i = \bP_{\theta^{(t)}}(X_i=1|Y_i)$. The intermediate quantity of the EM algorithm is given by
\begin{align*}
Q(\theta,\theta^{(t)}) &= - \frac{nd}{2} \log(2\pi)-\frac{n}2 \log\det\Sigma + \left(\sum_{i=1}^n\omega_t^i \right)\log \pi_1 + \sum_{i=1}^n\left(1 - \omega_t^i \right)\log (1-\pi_{1})\\
& \hspace{1cm}-  \frac{1}{2}\sum_{i=1}^n\omega_t^i \left(Y_i - \mu_{1}\right)^T\Sigma^{-1}\left(Y_i - \mu_{1}\right) -  \frac{1}{2}\sum_{i=1}^n(1-\omega_t^i )\left(Y_i - \mu_{-1}\right)^T\Sigma^{-1}\left(Y_i - \mu_{-1}\right)\eqsp.
\end{align*}
}
\item Compute $\theta^{(t+1)}$.

\vspace{.2cm}

{\em
The gradient of $Q(\theta,\theta^{(t)})$ with respect to $\theta$ is therefore given by
\begin{align*}
\frac{\partial Q(\theta,\theta^{(t)})}{\partial \pi_1} &= \frac{\sum_{i=1}^n\omega_t^i}{\pi_1} - \frac{n-\sum_{i=1}^n\omega_t^i}{1-\pi_{1}}\eqsp,\\
\frac{\partial Q(\theta,\theta^{(t)})}{\partial \mu_1} &= \sum_{i=1}^n\omega_t^i\left(2\Sigma^{-1}Y_i - 2\Sigma^{-1}\mu_{1}\right)\eqsp,\\
\frac{\partial Q(\theta,\theta^{(t)})}{\partial \mu_{-1}} &= \sum_{i=1}^n(1-\omega_t^i)\left(2\Sigma^{-1}Y_i - 2\Sigma^{-1}\mu_{-1}\right)\eqsp,\\
\frac{\partial Q(\theta,\theta^{(t)})}{\partial \Sigma^{-1}} &= \frac{n}{2}\Sigma -  \frac{1}{2}\sum_{i=1}^n\omega_t^i\left(Y_i - \mu_{1}\right)\left(Y_i - \mu_{1}\right)^\top -  \frac{1}{2}\sum_{i=1}^n(1-\omega_t^i)\left(Y_i - \mu_{-1}\right)\left(Y_i - \mu_{-1}\right)^\top\eqsp.
\end{align*}
Then, $\theta^{(t+1)}$ is defined as the only parameter such that all these equations are set to 0. It is given by
\begin{align*}
\widehat \pi_1^{(t+1)} &= \frac{1}{n}\sum_{i=1}^n\omega_t^i\eqsp,\\
\widehat \mu_1^{(t+1)} &= \frac{1}{\sum_{i=1}^n\omega_t^i}\sum_{i=1}^n\omega_t^i\,Y_i\eqsp,\\
\widehat\Sigma^{(t+1)} &= \frac{1}{n}\sum_{i=1}^n\omega_t^i\left(Y_i - \mu_{1}\right)\left(Y_i - \mu_{1}\right)^\top +  \frac{1}{n}\sum_{i=1}^n(1-\omega_t^i)\left(Y_i - \mu_{-1}\right)\left(Y_i - \mu_{-1}\right)^\top\eqsp.
\end{align*}}
\end{enumerate}


\end{document} 